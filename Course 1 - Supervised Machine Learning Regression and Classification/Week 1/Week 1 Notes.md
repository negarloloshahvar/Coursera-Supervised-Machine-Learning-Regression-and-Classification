# Week 1: Introduction to Machine Learning

## Chapter Objectives

- Define machine learning
- Define supervised learning
- Define unsupervised learning
- Write and run Python code in Jupyter Notebooks
- Define a regression model
- Implement and visualize a cost function
- Implement gradient descent
- Optimize a regression model using gradient descent

For linear regression, the model is represented by ğ‘“ğ‘¤,ğ‘(ğ‘¥)=ğ‘¤ğ‘¥ + ğ‘ 	 or ğ‘“(ğ‘¥)=ğ‘¤ğ‘¥ + ğ‘.
NumPy arrays have an attribute called ```shape``` that returns a tuple with each index having the number of corresponding elements.

## Notations

### General
| Notation| Description	Python (if applicable) |
| ---------| -----------------------------------|
| ğ‘ | scalar, non bold	|
| ğš |	vector, bold	|

### Regression		
- ğ± 	Training Example feature values (in this lab - Size (1000 sqft))	x_train
- ğ² 	Training Example targets (in this lab Price (1000s of dollars))	y_train
- ğ‘¥(ğ‘–) ,  ğ‘¦(ğ‘–) 	 ğ‘–ğ‘¡â„ Training Example	x_i, y_i
- m	Number of training examples	m
- ğ‘¤ 	parameter: weight	w
- ğ‘ 	parameter: bias	b
- ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–)) 	The result of the model evaluation at  ğ‘¥(ğ‘–)  parameterized by  ğ‘¤,ğ‘ :  ğ‘“ğ‘¤,ğ‘(ğ‘¥(ğ‘–))=ğ‘¤ğ‘¥(ğ‘–)+ğ‘ 	f_wb

## Cost Function

The cost function will tell us how well the model is doing so that we can try to get it to do better.

![image](https://user-images.githubusercontent.com/113103161/209154964-69601815-e54e-4887-a0f8-da489a85296f.png)

w and b are parameters of the model, adjusted as the model learns from the data. Theyâ€™re also referred to as â€œcoefficientsâ€ or â€œweightsâ€

## Gradient Descent
Gradient descent is an algorithm for finding values of parameters w and b that minimize the cost function J.

![image](https://user-images.githubusercontent.com/113103161/209239755-88f25ad6-c6b1-4685-9c06-dc5640ea08a6.png)

